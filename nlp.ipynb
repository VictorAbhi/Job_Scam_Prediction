{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_set/fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text columns, we can replace missing values with missing\n",
    "text_columns = ['company_profile', 'description', 'requirements', 'benefits']\n",
    "df[text_columns] = df[text_columns].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical columns, we can replace missing values with the relevant placeholders\n",
    "df['employment_type'] = df['employment_type'].fillna('Not Specified')\n",
    "df['required_experience'] = df['required_experience'].fillna('Not Specified')\n",
    "df['required_education'] = df['required_education'].fillna('Not Specified')\n",
    "df['industry'] = df['industry'].fillna('Not Specified')\n",
    "df['function'] = df['function'].fillna('Not Specified')\n",
    "df['location'] = df['location'].fillna('Unknown')\n",
    "df['department'] = df['department'].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['salary_range'] = df['salary_range'].fillna('Not Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['fraudulent'] = LabelEncoder().fit_transform(df['fraudulent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text columns\n",
    "df['text'] = df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df['text']\n",
    "y = df['fraudulent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['fraudulent'], test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train.values.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = np.array([preprocess_text(text[0]) for text in X_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100  # Set the maximum length for padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_resampled) \n",
    "X_sequences = tokenizer.texts_to_sequences(X_resampled)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "def build_lstm_model(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))  # Adjust dropout rate\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "766/766 [==============================] - 171s 215ms/step - loss: 0.1360 - accuracy: 0.9507 - val_loss: 0.1150 - val_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adabh\\Documents\\Job_Scam_Prediction\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 168s 220ms/step - loss: 0.0297 - accuracy: 0.9915 - val_loss: 0.0045 - val_accuracy: 0.9985 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "766/766 [==============================] - 172s 225ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.0089 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "766/766 [==============================] - 173s 226ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "766/766 [==============================] - 174s 227ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0117 - val_accuracy: 0.9982 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "766/766 [==============================] - 173s 226ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0097 - val_accuracy: 0.9949 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "766/766 [==============================] - 174s 227ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1768fcc4be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training LSTM Model with Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "lstm_model = build_lstm_model(max_length)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, save_format='h5')\n",
    "\n",
    "lstm_model.fit(X_padded, y_resampled, epochs=20, batch_size=32, validation_split=0.1,\n",
    "                callbacks=[early_stopping, reduce_lr, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data for prediction\n",
    "X_test_resampled = np.array([preprocess_text(text) for text in X_test])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_resampled)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 8s 71ms/step - loss: 0.1613 - accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16127221286296844, 0.9725950956344604]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "lstm_model.evaluate(X_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 70ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      3403\n",
      "           1       0.75      0.65      0.70       173\n",
      "\n",
      "    accuracy                           0.97      3576\n",
      "   macro avg       0.87      0.82      0.84      3576\n",
      "weighted avg       0.97      0.97      0.97      3576\n",
      "\n",
      "[[3366   37]\n",
      " [  61  112]]\n"
     ]
    }
   ],
   "source": [
    "# classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = lstm_model.predict(X_test_padded)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
